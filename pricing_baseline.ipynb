{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f022f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a notebook cell (prefix with !). If in VS Code terminal, run without !\n",
    "!pip install -q pandas numpy scikit-learn transformers sentence_transformers xgboost tqdm pillow requests torchvision accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14848115",
   "metadata": {},
   "source": [
    "IMPORTS AND BASIC CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05a36d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashidixit/Downloads/student_resource/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x131bb01f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code purpose: Imports and basic config\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast, CLIPProcessor, CLIPModel\n",
    "\n",
    "# Repro\n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "torch.manual_seed(RND)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a2405",
   "metadata": {},
   "source": [
    "PATHS AND USER CHOICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32d0fff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Code purpose: file paths and parameters\n",
    "TRAIN_CSV = 'dataset/train.csv'\n",
    "TEST_CSV  = 'dataset/test.csv'\n",
    "SAMPLE_OUT = 'dataset/sample_test_out.csv'\n",
    "OUT_FILE = 'test_out.csv'\n",
    "\n",
    "# How many images to download/process for quick multimodal experiments:\n",
    "IMAGE_LIMIT = 5000  # user-specified limit to check image effect quickly\n",
    "\n",
    "# CLIP device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114be739",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b6fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code purpose: load datasets\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c7b31",
   "metadata": {},
   "source": [
    "Helper functions: text cleaning & quantity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70b473d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code purpose: cleaning and extract numeric signals like quantity and units\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return ''\n",
    "    s = str(s)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)     # remove html\n",
    "    s = s.replace('-', ' ')\n",
    "    s = re.sub(r'[^A-Za-z0-9\\.\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def extract_quantity(s):\n",
    "    if pd.isna(s): return 1\n",
    "    s = str(s).lower()\n",
    "    # common patterns: \"pack of 12\", \"12 pack\", \"12 pcs\", \"12 pcs.\", \"pack of 12 pcs\"\n",
    "    m = re.search(r'(\\d{1,4})\\s*(?:pack|pcs|pieces|pc|units|unit|count)\\b', s)\n",
    "    if m: \n",
    "        try: return int(m.group(1))\n",
    "        except: pass\n",
    "    m2 = re.search(r'pack of\\s*(\\d{1,4})', s)\n",
    "    if m2:\n",
    "        try: return int(m2.group(1))\n",
    "        except: pass\n",
    "    # fallback: search any standalone integer that might indicate quantity\n",
    "    m3 = re.search(r'\\b(\\d{1,4})\\b', s)\n",
    "    if m3:\n",
    "        val = int(m3.group(1))\n",
    "        if val <= 1000 and val > 1:\n",
    "            return val\n",
    "    return 1\n",
    "\n",
    "def extract_units_value(s):\n",
    "    if pd.isna(s): return 0.0\n",
    "    s = str(s).lower()\n",
    "    # find patterns like \"500 ml\", \"0.5 l\", \"250g\", \"250 g\"\n",
    "    m = re.search(r'(\\d+\\.?\\d*)\\s*(ml|l|litre|litres|g|kg|gram|grams)\\b', s)\n",
    "    if m:\n",
    "        num = float(m.group(1))\n",
    "        unit = m.group(2)\n",
    "        # normalize units to ml or g (simple heuristics)\n",
    "        if unit in ['l','litre','litres']:\n",
    "            return num * 1000.0  # liters -> ml\n",
    "        if unit == 'ml':\n",
    "            return num\n",
    "        if unit in ['kg']:\n",
    "            return num * 1000.0  # kg -> g\n",
    "        if unit in ['g','gram','grams']:\n",
    "            return num\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5b075",
   "metadata": {},
   "source": [
    "Apply basic feature engineering on text for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "185b00da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_clean</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_value</th>\n",
       "      <th>text_len</th>\n",
       "      <th>kw_pack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>item name la victoria green taco sauce mild 12...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>item name salerno cookies the original butter ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>491</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>item name bear creek hearty soup bowl creamy c...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>item name judee s blue cheese powder 11.25 oz ...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>item name kedem sherry cooking wine 12.7 ounce...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                      catalog_clean  quantity  \\\n",
       "0      33127  item name la victoria green taco sauce mild 12...         0   \n",
       "1     198967  item name salerno cookies the original butter ...         0   \n",
       "2     261251  item name bear creek hearty soup bowl creamy c...         4   \n",
       "3      55858  item name judee s blue cheese powder 11.25 oz ...        25   \n",
       "4     292686  item name kedem sherry cooking wine 12.7 ounce...         0   \n",
       "\n",
       "   unit_value  text_len  kw_pack  \n",
       "0         0.0        84        1  \n",
       "1         0.0       491        1  \n",
       "2         0.0       315        1  \n",
       "3         0.0      1279        0  \n",
       "4         0.0       144        0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code purpose: create features from catalog_content for train and test\n",
    "for df in (train, test):\n",
    "    df['catalog_clean'] = df['catalog_content'].fillna('').apply(clean_text)\n",
    "    df['quantity'] = df['catalog_clean'].apply(extract_quantity)\n",
    "    df['unit_value'] = df['catalog_clean'].apply(extract_units_value)\n",
    "    df['text_len'] = df['catalog_clean'].apply(lambda x: len(x))\n",
    "    # keyword flags\n",
    "    for kw in ['premium','refill','combo','original','pack','bottle','set','new']:\n",
    "        df[f'kw_{kw}'] = df['catalog_clean'].apply(lambda s: 1 if kw in s else 0)\n",
    "\n",
    "# Quick sanity check\n",
    "train[['sample_id','catalog_clean','quantity','unit_value','text_len','kw_pack']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72720632",
   "metadata": {},
   "source": [
    "TARGET HANDLING AND OUTLIER CLIPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "258739af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.5 percentile price cap: 183.70050000000046\n"
     ]
    }
   ],
   "source": [
    "# Code purpose: clip extreme prices and compute log target for training stability\n",
    "# Find 99.5th percentile cap to limit huge outliers\n",
    "price_99_5 = train['price'].quantile(0.995)\n",
    "print(\"99.5 percentile price cap:\", price_99_5)\n",
    "\n",
    "# Create clipped target and log1p transform\n",
    "train['price_clipped'] = train['price'].clip(1, price_99_5)\n",
    "train['y_log1p'] = np.log1p(train['price_clipped'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298fc12",
   "metadata": {},
   "source": [
    "BERT TEXT EMBEDDING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90a810e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Computing MiniLM embeddings (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1172/1172 [12:11<00:00,  1.60it/s]\n",
      "Batches: 100%|██████████| 1172/1172 [13:56<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings computed and saved to disk!\n",
      "MiniLM embeddings shapes: (75000, 384) (75000, 384)\n"
     ]
    }
   ],
   "source": [
    "# Code Purpose: Generate or Load MiniLM embeddings for product texts (train + test)\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Embedding save paths\n",
    "train_emb_path = 'train_text_emb.npy'\n",
    "test_emb_path  = 'test_text_emb.npy'\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if os.path.exists(train_emb_path) and os.path.exists(test_emb_path):\n",
    "    print(\"✅ Loading embeddings from disk...\")\n",
    "    train_text_emb = np.load(train_emb_path)\n",
    "    test_text_emb  = np.load(test_emb_path)\n",
    "else:\n",
    "    print(\"⚡ Computing MiniLM embeddings (this may take a few minutes)...\")\n",
    "    # Load MiniLM model (fast, good performance)\n",
    "    text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
    "\n",
    "    # Convert to list\n",
    "    train_texts = train['catalog_clean'].tolist()\n",
    "    test_texts  = test['catalog_clean'].tolist()\n",
    "\n",
    "    # Encode in batches\n",
    "    train_text_emb = text_model.encode(\n",
    "        train_texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    test_text_emb = text_model.encode(\n",
    "        test_texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "    # Save embeddings for future runs\n",
    "    np.save(train_emb_path, train_text_emb)\n",
    "    np.save(test_emb_path, test_text_emb)\n",
    "    print(\"✅ Embeddings computed and saved to disk!\")\n",
    "\n",
    "print(\"MiniLM embeddings shapes:\", train_text_emb.shape, test_text_emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43dbc9",
   "metadata": {},
   "source": [
    "CLIP IMAGE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3e4ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading pre-downloaded train_clip_emb from train_clip_embeddings.npy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading & encoding images: 100%|██████████| 75000/75000 [5:03:54<00:00,  4.11it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 74993 / 75000 images\n",
      "📁 Embeddings saved to test_clip_embeddings.npy\n",
      "Train CLIP embeddings shape: (75000, 512)\n",
      "Test CLIP embeddings shape: (75000, 512)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Code Purpose: Generate & Cache CLIP Embeddings for All Images\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# ================================\n",
    "# ⚙️ Configuration\n",
    "# ================================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Paths to save precomputed embeddings\n",
    "train_clip_emb_path = \"train_clip_embeddings.npy\"\n",
    "test_clip_emb_path = \"test_clip_embeddings.npy\"\n",
    "\n",
    "# ================================\n",
    "# 📦 Load CLIP model and processor\n",
    "# ================================\n",
    "clip = CLIPModel.from_pretrained(clip_model_name).to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "\n",
    "# ================================\n",
    "# 🖼️ Helper function to download image\n",
    "# ================================\n",
    "def download_image(url, timeout=10):\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        return Image.open(BytesIO(resp.content)).convert('RGB')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ================================\n",
    "# 🧠 Extract CLIP embeddings for image URLs\n",
    "# ================================\n",
    "def extract_clip_embeddings_from_links(df, save_path, image_link_col='image_link', batch_size=64):\n",
    "    # ✅ Load from cache if available\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"✅ Found cached embeddings at {save_path}, loading...\")\n",
    "        return np.load(save_path)\n",
    "\n",
    "    # Otherwise compute embeddings\n",
    "    total_images = len(df)\n",
    "    embeddings = np.zeros((total_images, clip.config.projection_dim))\n",
    "    embeddings[:] = np.nan\n",
    "\n",
    "    processed = 0\n",
    "    for i in tqdm(range(total_images), desc=\"Downloading & encoding images\"):\n",
    "        url = df.iloc[i][image_link_col]\n",
    "        if pd.isna(url) or not isinstance(url, str) or len(url.strip()) == 0:\n",
    "            continue\n",
    "        img = download_image(url)\n",
    "        if img is None:\n",
    "            continue\n",
    "        try:\n",
    "            inputs = clip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                image_emb = clip.get_image_features(**inputs)  # shape: (1, 512)\n",
    "                image_emb = image_emb / image_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "                embeddings[i] = image_emb.cpu().numpy()\n",
    "            processed += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ Processed {processed} / {total_images} images\")\n",
    "    np.save(save_path, embeddings)  # 💾 Save embeddings\n",
    "    print(f\"📁 Embeddings saved to {save_path}\")\n",
    "    return embeddings\n",
    "\n",
    "# ================================\n",
    "# 🚀 Generate or load embeddings\n",
    "# ================================\n",
    "# Load train_clip_emb from the pre-downloaded file\n",
    "if os.path.exists(train_clip_emb_path):\n",
    "    print(f\"✅ Loading pre-downloaded train_clip_emb from {train_clip_emb_path}...\")\n",
    "    train_clip_emb = np.load(train_clip_emb_path)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"train_clip_emb_path not found at {train_clip_emb_path}. Please ensure it is downloaded.\")\n",
    "\n",
    "# Generate or load test_clip_emb\n",
    "test_clip_emb = extract_clip_embeddings_from_links(test, test_clip_emb_path)\n",
    "\n",
    "print(\"Train CLIP embeddings shape:\", train_clip_emb.shape)\n",
    "print(\"Test CLIP embeddings shape:\", test_clip_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e240c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 512)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clip_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fb529f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_clip_2.npy', test_clip_emb)  # Save test embeddings for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fe7b8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = np.load('test_clip_2.npy')\n",
    "i.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e7616",
   "metadata": {},
   "source": [
    "BUILD FEATURE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4523b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric feature shapes: (75000, 11) (75000, 11)\n",
      "Text + Numeric shapes: (75000, 395) (75000, 395)\n",
      "Multimodal (Text+Num+Image) shapes: (75000, 907) (75000, 907)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Numeric columns for feature engineering\n",
    "num_cols = ['quantity', 'unit_value', 'text_len'] + [f'kw_{kw}' for kw in [\n",
    "    'premium', 'refill', 'combo', 'original', 'pack', 'bottle', 'set', 'new'\n",
    "]]\n",
    "\n",
    "# Fill NaNs with 0 and convert to NumPy arrays\n",
    "X_num_train = train[num_cols].fillna(0).values\n",
    "X_num_test  = test[num_cols].fillna(0).values\n",
    "\n",
    "print(\"Numeric feature shapes:\", X_num_train.shape, X_num_test.shape)\n",
    "\n",
    "# 👇 Note: We now use `train_text_emb` and `test_text_emb` (MiniLM) instead of train_bert_emb\n",
    "X_text_train = np.hstack([train_text_emb, X_num_train])\n",
    "X_text_test  = np.hstack([test_text_emb, X_num_test])\n",
    "print(\"Text + Numeric shapes:\", X_text_train.shape, X_text_test.shape)\n",
    "\n",
    "# 🖼 If using CLIP image embeddings — align array shapes properly.\n",
    "# Make sure `train_clip_emb` and `test_clip_emb` already exist.\n",
    "# If some rows don’t have image embeddings, fill with zeros or np.nan (then handle accordingly).\n",
    "\n",
    "# Example: fill missing image embeddings with zeros to maintain shape\n",
    "if train_clip_emb is not None and test_clip_emb is not None:\n",
    "    X_mm_train = np.hstack([train_text_emb, X_num_train, train_clip_emb])\n",
    "    X_mm_test  = np.hstack([test_text_emb, X_num_test, test_clip_emb])\n",
    "    print(\"Multimodal (Text+Num+Image) shapes:\", X_mm_train.shape, X_mm_test.shape)\n",
    "else:\n",
    "    print(\"⚠️ No CLIP embeddings found — skipping multimodal combination.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4e834",
   "metadata": {},
   "source": [
    "SMAPE METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c23b6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code purpose: SMAPE\n",
    "def smape(actual, pred):\n",
    "    actual = np.array(actual, dtype=float)\n",
    "    pred   = np.array(pred, dtype=float)\n",
    "    denom = (np.abs(actual) + np.abs(pred)) / 2.0\n",
    "    # avoid div by 0\n",
    "    mask = denom == 0\n",
    "    denom[mask] = 1.0\n",
    "    diff = np.abs(actual - pred) / denom\n",
    "    return 100.0 * np.mean(diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3ac74",
   "metadata": {},
   "source": [
    "TRAIN TEXT ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53143a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "[0]\tvalidation_0-rmse:2.35843\n",
      "[50]\tvalidation_0-rmse:0.97503\n",
      "[100]\tvalidation_0-rmse:0.83255\n",
      "[150]\tvalidation_0-rmse:0.81080\n",
      "[200]\tvalidation_0-rmse:0.80033\n",
      "[250]\tvalidation_0-rmse:0.79324\n",
      "[300]\tvalidation_0-rmse:0.78728\n",
      "[350]\tvalidation_0-rmse:0.78243\n",
      "[400]\tvalidation_0-rmse:0.77872\n",
      "[450]\tvalidation_0-rmse:0.77577\n",
      "[500]\tvalidation_0-rmse:0.77320\n",
      "[550]\tvalidation_0-rmse:0.77081\n",
      "[600]\tvalidation_0-rmse:0.76860\n",
      "[650]\tvalidation_0-rmse:0.76644\n",
      "[700]\tvalidation_0-rmse:0.76482\n",
      "[750]\tvalidation_0-rmse:0.76318\n",
      "[800]\tvalidation_0-rmse:0.76174\n",
      "[850]\tvalidation_0-rmse:0.76048\n",
      "[900]\tvalidation_0-rmse:0.75909\n",
      "[950]\tvalidation_0-rmse:0.75800\n",
      "[999]\tvalidation_0-rmse:0.75670\n",
      "Text-only validation SMAPE: 59.1547%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure required variables are defined\n",
    "if 'X_text_train' not in locals():\n",
    "    raise ValueError(\"X_text_train is not defined. Ensure it is created in the 'BUILD FEATURE METRICS' section.\")\n",
    "if 'train' not in locals() or 'y_log1p' not in train.columns:\n",
    "    raise ValueError(\"The 'train' DataFrame or the 'y_log1p' column is not defined.\")\n",
    "if 'RND' not in locals():\n",
    "    RND = 42  # Default random seed\n",
    "if 'smape' not in locals():\n",
    "    def smape(actual, pred):\n",
    "        actual = np.array(actual, dtype=float)\n",
    "        pred = np.array(pred, dtype=float)\n",
    "        denom = (np.abs(actual) + np.abs(pred)) / 2.0\n",
    "        denom[denom == 0] = 1e-8  # Avoid division by zero\n",
    "        diff = np.abs(actual - pred) / denom\n",
    "        return 100.0 * np.mean(diff)\n",
    "\n",
    "# Define features and target\n",
    "X = X_text_train\n",
    "y = train['y_log1p']  # Target variable\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.15, random_state=RND)\n",
    "\n",
    "# Define XGBoost parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': RND,\n",
    "    'verbosity': 1,\n",
    "    'early_stopping_rounds': 50  # Early stopping moved here\n",
    "}\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "model_text = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "print(\"Training XGBoost model...\")\n",
    "model_text.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=50  # Use verbose_eval for newer XGBoost versions\n",
    ")\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred_log = model_text.predict(X_val)\n",
    "y_val_pred = np.expm1(y_val_pred_log)  # Invert log1p transformation\n",
    "y_val_true = np.expm1(y_val)  # Invert log1p transformation\n",
    "\n",
    "# Calculate SMAPE\n",
    "validation_smape = smape(y_val_true, y_val_pred)\n",
    "print(f\"Text-only validation SMAPE: {validation_smape:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e2dd1",
   "metadata": {},
   "source": [
    "MULTIMODAL EXPERIMNET ON IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51418304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 rows with complete image embeddings.\n",
      "Sufficient data available. Preparing multimodal model training...\n",
      "Training multimodal XGBoost model...\n",
      "[0]\tvalidation_0-rmse:2.31631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashidixit/Downloads/student_resource/.venv/lib/python3.13/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalidation_0-rmse:0.81540\n",
      "[100]\tvalidation_0-rmse:0.77457\n",
      "[150]\tvalidation_0-rmse:0.76148\n",
      "[200]\tvalidation_0-rmse:0.75309\n",
      "[250]\tvalidation_0-rmse:0.74637\n",
      "[300]\tvalidation_0-rmse:0.74168\n",
      "[350]\tvalidation_0-rmse:0.73769\n",
      "[400]\tvalidation_0-rmse:0.73429\n",
      "[450]\tvalidation_0-rmse:0.73145\n",
      "[500]\tvalidation_0-rmse:0.72956\n",
      "[550]\tvalidation_0-rmse:0.72812\n",
      "[600]\tvalidation_0-rmse:0.72644\n",
      "[650]\tvalidation_0-rmse:0.72499\n",
      "[700]\tvalidation_0-rmse:0.72379\n",
      "[750]\tvalidation_0-rmse:0.72253\n",
      "[799]\tvalidation_0-rmse:0.72149\n",
      "\n",
      "Multimodal validation SMAPE (on image subset): 55.9991%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "MIN_SAMPLES_FOR_TRAINING = 200\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "XGB_N_ESTIMATORS = 800\n",
    "XGB_EARLY_STOPPING_ROUNDS = 40\n",
    "XGB_VERBOSE_INTERVAL = 50\n",
    "\n",
    "# --- Helper Function ---\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    denominator[denominator == 0] = 1e-8  # Avoid division by zero\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "# --- Data Validation and Preparation ---\n",
    "if 'train_clip_emb' not in locals() or 'X_mm_train' not in locals() or 'train' not in locals():\n",
    "    raise ValueError(\"Ensure 'train_clip_emb', 'X_mm_train', and 'train' are defined before running this block.\")\n",
    "\n",
    "# 1. Handle missing values in `train_clip_emb`\n",
    "train_clip_emb = np.nan_to_num(train_clip_emb)\n",
    "\n",
    "# 2. Find rows where all image embedding values are finite\n",
    "valid_img_mask = np.isfinite(train_clip_emb).all(axis=1)\n",
    "num_valid_rows = np.sum(valid_img_mask)\n",
    "\n",
    "print(f\"Found {num_valid_rows} rows with complete image embeddings.\")\n",
    "\n",
    "# 3. Check if we have enough valid data to proceed with training\n",
    "if num_valid_rows >= MIN_SAMPLES_FOR_TRAINING:\n",
    "    print(\"Sufficient data available. Preparing multimodal model training...\")\n",
    "\n",
    "    # 4. Prepare dataset using the boolean mask\n",
    "    X_mm_valid = X_mm_train[valid_img_mask]\n",
    "    y_mm_valid = train.loc[valid_img_mask, 'y_log1p'].values\n",
    "\n",
    "    # 5. Split the valid data into training and validation sets\n",
    "    X_tr_m, X_val_m, y_tr_m, y_val_m = train_test_split(\n",
    "        X_mm_valid,\n",
    "        y_mm_valid,\n",
    "        test_size=TEST_SPLIT_RATIO,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # --- Model Training and Evaluation ---\n",
    "    xgb_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_estimators': XGB_N_ESTIMATORS,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "\n",
    "    model_mm = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "    print(\"Training multimodal XGBoost model...\")\n",
    "    model_mm.fit(\n",
    "        X_tr_m,\n",
    "        y_tr_m,\n",
    "        eval_set=[(X_val_m, y_val_m)],\n",
    "        early_stopping_rounds=XGB_EARLY_STOPPING_ROUNDS,\n",
    "        verbose=XGB_VERBOSE_INTERVAL\n",
    "    )\n",
    "\n",
    "    # 6. Evaluate the model on the validation set\n",
    "    y_val_mm_pred = np.expm1(model_mm.predict(X_val_m))\n",
    "    y_val_mm_true = np.expm1(y_val_m)\n",
    "\n",
    "    validation_smape = smape(y_val_mm_true, y_val_mm_pred)\n",
    "    print(f\"\\nMultimodal validation SMAPE (on image subset): {validation_smape:.4f}%\")\n",
    "\n",
    "else:\n",
    "    print(f\"Not enough image-enabled rows to train multimodal model reliably. \"\n",
    "          f\"Required: {MIN_SAMPLES_FOR_TRAINING}, Found: {num_valid_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7486446",
   "metadata": {},
   "source": [
    "FINAL TRAINING AND TEST PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "532c0a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:2.32011\n",
      "[50]\tvalidation_0-rmse:0.81481\n",
      "[100]\tvalidation_0-rmse:0.74960\n",
      "[150]\tvalidation_0-rmse:0.71670\n",
      "[200]\tvalidation_0-rmse:0.69017\n",
      "[250]\tvalidation_0-rmse:0.66829\n",
      "[300]\tvalidation_0-rmse:0.64810\n",
      "[350]\tvalidation_0-rmse:0.62948\n",
      "[400]\tvalidation_0-rmse:0.61336\n",
      "[450]\tvalidation_0-rmse:0.59745\n",
      "[500]\tvalidation_0-rmse:0.58274\n",
      "[550]\tvalidation_0-rmse:0.56906\n",
      "[600]\tvalidation_0-rmse:0.55588\n",
      "[650]\tvalidation_0-rmse:0.54278\n",
      "[700]\tvalidation_0-rmse:0.53105\n",
      "[750]\tvalidation_0-rmse:0.51896\n",
      "[800]\tvalidation_0-rmse:0.50769\n",
      "[850]\tvalidation_0-rmse:0.49665\n",
      "[900]\tvalidation_0-rmse:0.48577\n",
      "[950]\tvalidation_0-rmse:0.47572\n",
      "[1000]\tvalidation_0-rmse:0.46568\n",
      "[1050]\tvalidation_0-rmse:0.45619\n",
      "[1100]\tvalidation_0-rmse:0.44690\n",
      "[1150]\tvalidation_0-rmse:0.43792\n",
      "[1199]\tvalidation_0-rmse:0.42966\n",
      "Text-only prediction saved to test_out_text_only.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>14.890229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>19.925413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>20.500971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>13.670382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>18.845936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id      price\n",
       "0     100179  14.890229\n",
       "1     245611  19.925413\n",
       "2     146263  20.500971\n",
       "3      95658  13.670382\n",
       "4      36806  18.845936"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain text-only on full training data and predict on entire test\n",
    "model_text_full = xgb.XGBRegressor(**{**xgb_params, 'n_estimators': 1200})\n",
    "\n",
    "# Handle missing values in the training and test sets\n",
    "X_text_train = np.nan_to_num(X_text_train)\n",
    "X_text_test = np.nan_to_num(X_text_test)\n",
    "\n",
    "# Fit the model\n",
    "model_text_full.fit(\n",
    "    X_text_train,\n",
    "    train['y_log1p'].values,\n",
    "    eval_set=[(X_text_train, train['y_log1p'].values)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Predict and invert log\n",
    "y_test_pred_log = model_text_full.predict(X_text_test)\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "\n",
    "# Clip predictions to avoid extremes\n",
    "y_test_pred = np.clip(y_test_pred, 1.0, price_99_5)\n",
    "\n",
    "# Save text-only submission\n",
    "sub_text = pd.DataFrame({'sample_id': test['sample_id'], 'price': y_test_pred})\n",
    "sub_text.to_csv('test_out_text_only.csv', index=False)\n",
    "\n",
    "print(\"Text-only prediction saved to test_out_text_only.csv\")\n",
    "sub_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd1e50",
   "metadata": {},
   "source": [
    "trying different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ec19b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined multimodal feature shape: (75000, 907)\n",
      "Training LightGBM model on multimodal data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 229235\n",
      "[LightGBM] [Info] Number of data points in the train set: 63750, number of used features: 907\n",
      "[LightGBM] [Info] Start training from score 2.739550\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 0.525212\n",
      "Multimodal validation SMAPE: 56.3305%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashidixit/Downloads/student_resource/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "# --- Configuration ---\n",
    "RANDOM_STATE = 42\n",
    "TEST_SPLIT_RATIO = 0.15\n",
    "\n",
    "# --- Helper Function ---\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    denominator[denominator == 0] = 1e-8  # Avoid division by zero\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "# --- Combine Text and Image Features ---\n",
    "# Ensure train_clip_emb and X_text_train are already loaded\n",
    "if 'train_clip_emb' not in locals() or 'X_text_train' not in locals():\n",
    "    raise ValueError(\"Ensure 'train_clip_emb' and 'X_text_train' are defined before running this block.\")\n",
    "\n",
    "# Combine text and image embeddings\n",
    "X_mm_train = np.hstack([X_text_train, train_clip_emb])\n",
    "print(\"Combined multimodal feature shape:\", X_mm_train.shape)\n",
    "\n",
    "# Define target variable\n",
    "y = train['y_log1p'].values\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_mm_train, y, test_size=TEST_SPLIT_RATIO, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- Train LightGBM Model ---\n",
    "lgb_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 31,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "model_lgb = LGBMRegressor(**lgb_params)\n",
    "\n",
    "print(\"Training LightGBM model on multimodal data...\")\n",
    "model_lgb.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[early_stopping(stopping_rounds=50, verbose=50)]\n",
    ")\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "y_val_pred_log = model_lgb.predict(X_val)\n",
    "y_val_pred = np.expm1(y_val_pred_log)  # Invert log1p transformation\n",
    "y_val_true = np.expm1(y_val)  # Invert log1p transformation\n",
    "\n",
    "# Calculate SMAPE\n",
    "validation_smape = smape(y_val_true, y_val_pred)\n",
    "print(f\"Multimodal validation SMAPE: {validation_smape:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "734def17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp313-cp313-macosx_11_0_universal2.whl.metadata (1.4 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting matplotlib (from catboost)\n",
      "  Downloading matplotlib-3.10.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in ./.venv/lib/python3.13/site-packages (from catboost) (2.3.3)\n",
      "Requirement already satisfied: pandas>=0.24 in ./.venv/lib/python3.13/site-packages (from catboost) (2.3.3)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.13/site-packages (from catboost) (1.16.2)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.13/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->catboost)\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->catboost)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->catboost)\n",
      "  Using cached fonttools-4.60.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->catboost)\n",
      "  Using cached kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib->catboost) (11.3.0)\n",
      "Collecting pyparsing>=3 (from matplotlib->catboost)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-2.8.0-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading catboost-1.2.8-cp313-cp313-macosx_11_0_universal2.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading matplotlib-3.10.7-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.8.0-py3-none-any.whl (415 kB)\n",
      "Installing collected packages: pyparsing, narwhals, kiwisolver, graphviz, fonttools, cycler, contourpy, plotly, matplotlib, catboost\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [catboost]/10\u001b[0m [catboost]b]\n",
      "\u001b[1A\u001b[2KSuccessfully installed catboost-1.2.8 contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 graphviz-0.21 kiwisolver-1.4.9 matplotlib-3.10.7 narwhals-2.8.0 plotly-6.3.1 pyparsing-3.2.5\n",
      "Training CatBoost model on multimodal data...\n",
      "0:\tlearn: 0.9264920\ttest: 0.9404136\tbest: 0.9404136 (0)\ttotal: 160ms\tremaining: 2m 39s\n",
      "50:\tlearn: 0.8159190\ttest: 0.8368420\tbest: 0.8368420 (50)\ttotal: 3.96s\tremaining: 1m 13s\n",
      "100:\tlearn: 0.7902619\ttest: 0.8152969\tbest: 0.8152969 (100)\ttotal: 7.29s\tremaining: 1m 4s\n",
      "150:\tlearn: 0.7751984\ttest: 0.8040440\tbest: 0.8040440 (150)\ttotal: 9.64s\tremaining: 54.2s\n",
      "200:\tlearn: 0.7633932\ttest: 0.7957169\tbest: 0.7957169 (200)\ttotal: 12.1s\tremaining: 47.9s\n",
      "250:\tlearn: 0.7515576\ttest: 0.7879254\tbest: 0.7879254 (250)\ttotal: 14.4s\tremaining: 43.1s\n",
      "300:\tlearn: 0.7396497\ttest: 0.7810174\tbest: 0.7810174 (300)\ttotal: 16.8s\tremaining: 39.1s\n",
      "350:\tlearn: 0.7293460\ttest: 0.7757719\tbest: 0.7757719 (350)\ttotal: 19.2s\tremaining: 35.5s\n",
      "400:\tlearn: 0.7198624\ttest: 0.7712539\tbest: 0.7712539 (400)\ttotal: 21.5s\tremaining: 32.1s\n",
      "450:\tlearn: 0.7110882\ttest: 0.7670323\tbest: 0.7670323 (450)\ttotal: 23.9s\tremaining: 29.1s\n",
      "500:\tlearn: 0.7027150\ttest: 0.7633670\tbest: 0.7633670 (500)\ttotal: 26.3s\tremaining: 26.2s\n",
      "550:\tlearn: 0.6947466\ttest: 0.7601894\tbest: 0.7601894 (550)\ttotal: 29s\tremaining: 23.7s\n",
      "600:\tlearn: 0.6874068\ttest: 0.7574212\tbest: 0.7574212 (600)\ttotal: 31.7s\tremaining: 21.1s\n",
      "650:\tlearn: 0.6803890\ttest: 0.7549696\tbest: 0.7549696 (650)\ttotal: 34.1s\tremaining: 18.3s\n",
      "700:\tlearn: 0.6737043\ttest: 0.7529296\tbest: 0.7529296 (700)\ttotal: 36.4s\tremaining: 15.5s\n",
      "750:\tlearn: 0.6672800\ttest: 0.7510647\tbest: 0.7510647 (750)\ttotal: 38.8s\tremaining: 12.9s\n",
      "800:\tlearn: 0.6609278\ttest: 0.7490914\tbest: 0.7490914 (800)\ttotal: 41.2s\tremaining: 10.2s\n",
      "850:\tlearn: 0.6548573\ttest: 0.7473018\tbest: 0.7473018 (850)\ttotal: 43.8s\tremaining: 7.68s\n",
      "900:\tlearn: 0.6488992\ttest: 0.7455933\tbest: 0.7455933 (900)\ttotal: 46.2s\tremaining: 5.08s\n",
      "950:\tlearn: 0.6432638\ttest: 0.7440933\tbest: 0.7440933 (950)\ttotal: 48.6s\tremaining: 2.5s\n",
      "999:\tlearn: 0.6377720\ttest: 0.7429135\tbest: 0.7429135 (999)\ttotal: 50.9s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7429134636\n",
      "bestIteration = 999\n",
      "\n",
      "Multimodal validation SMAPE with CatBoost: 58.0214%\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- Train CatBoost Model ---\n",
    "cat_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'eval_metric': 'RMSE',\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 50\n",
    "}\n",
    "\n",
    "model_cat = CatBoostRegressor(**cat_params)\n",
    "\n",
    "print(\"Training CatBoost model on multimodal data...\")\n",
    "model_cat.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    eval_set=(X_val, y_val),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "y_val_pred_log = model_cat.predict(X_val)\n",
    "y_val_pred = np.expm1(y_val_pred_log)  # Invert log1p transformation\n",
    "y_val_true = np.expm1(y_val)  # Invert log1p transformation\n",
    "\n",
    "# Calculate SMAPE\n",
    "validation_smape = smape(y_val_true, y_val_pred)\n",
    "print(f\"Multimodal validation SMAPE with CatBoost: {validation_smape:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0794220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./.venv/lib/python3.13/site-packages (from tensorflow) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.75.1-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.3.3)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp313-cp313-macosx_10_13_universal2.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-macosx_12_0_arm64.whl (200.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.7/200.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.75.1-cp313-cp313-macosx_11_0_universal2.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp313-cp313-macosx_10_13_universal2.whl (663 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.8/663.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.0-cp313-cp313-macosx_14_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wrapt-1.17.3-cp313-cp313-macosx_11_0_arm64.whl (39 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp313-cp313-macosx_11_0_arm64.whl (354 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [tensorflow]5\u001b[0m [tensorflow]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.1 h5py-3.15.0 keras-3.11.3 libclang-18.1.1 markdown-3.9 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.32.1 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc7ff20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[32m      6\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mmodel.h5\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Path to your model file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load the test image embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/student_resource/.venv/lib/python3.13/site-packages/keras/src/saving/saving_api.py:196\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib.load_model(\n\u001b[32m    190\u001b[39m         filepath,\n\u001b[32m    191\u001b[39m         custom_objects=custom_objects,\n\u001b[32m    192\u001b[39m         \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m,\n\u001b[32m    193\u001b[39m         safe_mode=safe_mode,\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith((\u001b[33m\"\u001b[39m\u001b[33m.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.hdf5\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    204\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/student_resource/.venv/lib/python3.13/site-packages/keras/src/legacy/saving/legacy_h5_format.py:118\u001b[39m, in \u001b[36mload_model_from_hdf5\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    116\u001b[39m opened_new_file = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py.File)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     f = \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    120\u001b[39m     f = filepath\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/student_resource/.venv/lib/python3.13/site-packages/h5py/_hl/files.py:566\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, track_times, **kwds)\u001b[39m\n\u001b[32m    557\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    558\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    559\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    560\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    561\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    562\u001b[39m                      **kwds)\n\u001b[32m    563\u001b[39m     fcpl = make_fcpl(track_order=track_order, track_times=track_times,\n\u001b[32m    564\u001b[39m                      fs_strategy=fs_strategy, fs_persist=fs_persist,\n\u001b[32m    565\u001b[39m                      fs_threshold=fs_threshold, fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/student_resource/.venv/lib/python3.13/site-packages/h5py/_hl/files.py:241\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[32m    240\u001b[39m         flags |= h5f.ACC_SWMR_READ\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    243\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:104\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = \"model.h5\"  # Path to your model file\n",
    "model = load_model(model_path)\n",
    "print(f\"✅ Loaded model from {model_path}\")\n",
    "\n",
    "# Load the test image embeddings\n",
    "test_clip_emb_path = \"test_clip_2.npy\"  # Path to your test embeddings file\n",
    "test_clip_emb = np.load(test_clip_emb_path)\n",
    "print(f\"✅ Loaded test embeddings from {test_clip_emb_path}\")\n",
    "print(f\"Test embeddings shape: {test_clip_emb.shape}\")\n",
    "\n",
    "# Ensure the test embeddings are valid\n",
    "if not np.isfinite(test_clip_emb).all():\n",
    "    raise ValueError(\"Test embeddings contain NaN or infinite values. Please clean the data.\")\n",
    "\n",
    "# Predict using the loaded model\n",
    "print(\"⚡ Making predictions on test data...\")\n",
    "test_predictions_log = model.predict(test_clip_emb)  # Predictions in log scale\n",
    "test_predictions = np.expm1(test_predictions_log)  # Invert log1p transformation\n",
    "\n",
    "# Clip predictions to avoid extreme values\n",
    "price_99_5 = train['price'].quantile(0.995)  # Assuming `train` DataFrame is loaded\n",
    "test_predictions = np.clip(test_predictions, 1.0, price_99_5)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "output_file = \"test_predictions.csv\"\n",
    "test_sample_ids = test['sample_id']  # Assuming `test` DataFrame is loaded\n",
    "submission = pd.DataFrame({'sample_id': test_sample_ids, 'price': test_predictions.flatten()})\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Predictions saved to {output_file}\")\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f937e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model from /Users/rashidixit/Downloads/student_resource/my_model (1).h5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">918,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │       \u001b[38;5;34m918,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,582,081</span> (6.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,582,081\u001b[0m (6.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,578,497</span> (6.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,578,497\u001b[0m (6.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> (14.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,584\u001b[0m (14.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Correct the path to the model file\n",
    "model_path = \"/Users/rashidixit/Downloads/student_resource/my_model (1).h5\"  # Absolute path to your model file\n",
    "model = load_model(model_path, compile=False)\n",
    "print(f\"✅ Loaded model from {model_path}\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d1a9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test feature shape: (75000, 896)\n",
      "⚡ Making predictions on test data...\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "✅ Predictions saved to test_out_new.csv\n"
     ]
    }
   ],
   "source": [
    "X_test_text = np.load('test_text_emb.npy')\n",
    "X_test_img = np.load('test_clip_2.npy')\n",
    "X_test_img = np.nan_to_num(X_test_img)  # Handle NaNs\n",
    "\n",
    "X_test = np.hstack([X_test_text, X_test_img])\n",
    "print(\"Test feature shape:\", X_test.shape)\n",
    "\n",
    "#predict using the loaded model\n",
    "print(\"⚡ Making predictions on test data...\")\n",
    "test_predictions = model.predict(X_test)  # Predictions in log scale\n",
    "  # Invert log1p transformation\n",
    "\n",
    "test_predictions.shape\n",
    "\n",
    "test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "t = pd.read_csv('dataset/test.csv')\n",
    "submission = pd.DataFrame({'sample_id': t['sample_id'], 'price': test_predictions.flatten()})\n",
    "submission.to_csv('test_out_new.csv', index=False)\n",
    "print(f\"✅ Predictions saved to test_out_new.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70d344",
   "metadata": {},
   "source": [
    "#Fine tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34f73bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Data ---\n",
      "Data shape after dropping NaNs: (74984, 896)\n",
      "Scaler saved to /Users/rashidixit/Downloads/student_resource/scaler.pkl\n",
      "Log-transform of target variable complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">918,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_31          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_32          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m918,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_31          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_32          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,615,361</span> (6.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,615,361\u001b[0m (6.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,611,521</span> (6.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,611,521\u001b[0m (6.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> (15.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,840\u001b[0m (15.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training on 59987 samples ---\n",
      "Epoch 1/100\n",
      "\u001b[1m465/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 43.2964"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 30.2729 - val_loss: 25.5706 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m465/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 23.6113"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 23.3676 - val_loss: 23.0918 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "\u001b[1m465/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 22.3437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 22.4197 - val_loss: 22.3812 - learning_rate: 0.0100\n",
      "Epoch 4/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 21.8029 - val_loss: 22.7967 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 21.3975 - val_loss: 23.0633 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 20.9989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 21.0092 - val_loss: 21.9419 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 20.7750 - val_loss: 22.1192 - learning_rate: 0.0100\n",
      "Epoch 8/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 20.5395 - val_loss: 22.0654 - learning_rate: 0.0100\n",
      "Epoch 9/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 20.4063 - val_loss: 22.2274 - learning_rate: 0.0100\n",
      "Epoch 10/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 20.2719 - val_loss: 22.0557 - learning_rate: 0.0100\n",
      "Epoch 11/100\n",
      "\u001b[1m466/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 19.9617\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 20.2703 - val_loss: 22.6371 - learning_rate: 0.0100\n",
      "Epoch 12/100\n",
      "\u001b[1m465/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 19.3639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 19.2798 - val_loss: 21.7161 - learning_rate: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 18.7615 - val_loss: 21.9148 - learning_rate: 0.0050\n",
      "Epoch 14/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 18.6913 - val_loss: 22.1212 - learning_rate: 0.0050\n",
      "Epoch 15/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 18.4863 - val_loss: 22.1903 - learning_rate: 0.0050\n",
      "Epoch 16/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 18.3766 - val_loss: 22.4088 - learning_rate: 0.0050\n",
      "Epoch 17/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 17.9517\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 18.1959 - val_loss: 22.3992 - learning_rate: 0.0050\n",
      "Epoch 18/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 17.3009 - val_loss: 21.9127 - learning_rate: 0.0025\n",
      "Epoch 19/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 16.8295 - val_loss: 21.8833 - learning_rate: 0.0025\n",
      "Epoch 20/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 16.5323 - val_loss: 22.0047 - learning_rate: 0.0025\n",
      "Epoch 21/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 16.3081 - val_loss: 21.8822 - learning_rate: 0.0025\n",
      "Epoch 22/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 16.0605\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 16.1066 - val_loss: 21.9197 - learning_rate: 0.0025\n",
      "Epoch 23/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 15.5151 - val_loss: 21.8075 - learning_rate: 0.0012\n",
      "Epoch 24/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 15.2383 - val_loss: 21.7408 - learning_rate: 0.0012\n",
      "Epoch 25/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.0218"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 15.0760 - val_loss: 21.6886 - learning_rate: 0.0012\n",
      "Epoch 26/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 14.8872 - val_loss: 21.7014 - learning_rate: 0.0012\n",
      "Epoch 27/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 14.7214 - val_loss: 21.6978 - learning_rate: 0.0012\n",
      "Epoch 28/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 14.5587 - val_loss: 21.7056 - learning_rate: 0.0012\n",
      "Epoch 29/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 14.4013 - val_loss: 21.7805 - learning_rate: 0.0012\n",
      "Epoch 30/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 14.2232\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 14.2722 - val_loss: 21.7239 - learning_rate: 0.0012\n",
      "Epoch 31/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 13.9381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - loss: 13.9292 - val_loss: 21.6512 - learning_rate: 6.2500e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 13.8420"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - loss: 13.8380 - val_loss: 21.6161 - learning_rate: 6.2500e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 13.6958 - val_loss: 21.6197 - learning_rate: 6.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - loss: 13.6105 - val_loss: 21.6271 - learning_rate: 6.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m467/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 13.6071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 13.5469 - val_loss: 21.6092 - learning_rate: 6.2500e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - loss: 13.3735 - val_loss: 21.6181 - learning_rate: 6.2500e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 13.3231 - val_loss: 21.6283 - learning_rate: 6.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 13.2599 - val_loss: 21.6390 - learning_rate: 6.2500e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 13.2197"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 13.2047 - val_loss: 21.5958 - learning_rate: 6.2500e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 13.1571 - val_loss: 21.6404 - learning_rate: 6.2500e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 13.0739"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 13.1133 - val_loss: 21.5705 - learning_rate: 6.2500e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 13.0117 - val_loss: 21.6489 - learning_rate: 6.2500e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12.9807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 12.9685 - val_loss: 21.5372 - learning_rate: 6.2500e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 12.8768 - val_loss: 21.5634 - learning_rate: 6.2500e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 12.6716"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 12.7888 - val_loss: 21.5082 - learning_rate: 6.2500e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 12.7571 - val_loss: 21.5348 - learning_rate: 6.2500e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 12.7110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 12.7077 - val_loss: 21.4867 - learning_rate: 6.2500e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 12.5223 - val_loss: 21.5623 - learning_rate: 6.2500e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 12.6004 - val_loss: 21.4877 - learning_rate: 6.2500e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 12.5370 - val_loss: 21.5178 - learning_rate: 6.2500e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 12.4492 - val_loss: 21.4969 - learning_rate: 6.2500e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 12.3813\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 12.4091 - val_loss: 21.5198 - learning_rate: 6.2500e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 12.2600 - val_loss: 21.4888 - learning_rate: 3.1250e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 12.1919"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 12.1881 - val_loss: 21.4826 - learning_rate: 3.1250e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 12.1111 - val_loss: 21.5336 - learning_rate: 3.1250e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 12.1298 - val_loss: 21.5228 - learning_rate: 3.1250e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 12.0715 - val_loss: 21.4827 - learning_rate: 3.1250e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.9858"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 12.0598 - val_loss: 21.4691 - learning_rate: 3.1250e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m467/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.8667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.9954 - val_loss: 21.4691 - learning_rate: 3.1250e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.9831 - val_loss: 21.4775 - learning_rate: 3.1250e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m466/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.8342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.9409 - val_loss: 21.4600 - learning_rate: 3.1250e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.9279"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.8653 - val_loss: 21.4487 - learning_rate: 3.1250e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.9192 - val_loss: 21.4623 - learning_rate: 3.1250e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.8270 - val_loss: 21.4516 - learning_rate: 3.1250e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m468/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11.8285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.8253 - val_loss: 21.4016 - learning_rate: 3.1250e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.7822 - val_loss: 21.4409 - learning_rate: 3.1250e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.7241 - val_loss: 21.4386 - learning_rate: 3.1250e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 11.7412 - val_loss: 21.4409 - learning_rate: 3.1250e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 11.6271 - val_loss: 21.4292 - learning_rate: 3.1250e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m466/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 11.6033\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 11.6086 - val_loss: 21.4440 - learning_rate: 3.1250e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.5800 - val_loss: 21.4254 - learning_rate: 1.5625e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.6150 - val_loss: 21.4207 - learning_rate: 1.5625e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.4976 - val_loss: 21.4259 - learning_rate: 1.5625e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 11.5115 - val_loss: 21.4262 - learning_rate: 1.5625e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m466/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 11.5035\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.4954 - val_loss: 21.4156 - learning_rate: 1.5625e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.5426 - val_loss: 21.4041 - learning_rate: 7.8125e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m467/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 11.5579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.5158 - val_loss: 21.4007 - learning_rate: 7.8125e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - loss: 11.4412 - val_loss: 21.4140 - learning_rate: 7.8125e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.3867 - val_loss: 21.4051 - learning_rate: 7.8125e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 11.2723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.3371 - val_loss: 21.3957 - learning_rate: 7.8125e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.3970 - val_loss: 21.4167 - learning_rate: 7.8125e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 11.3128 - val_loss: 21.4055 - learning_rate: 7.8125e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.3767 - val_loss: 21.4072 - learning_rate: 7.8125e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.4000 - val_loss: 21.4002 - learning_rate: 7.8125e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m467/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 11.3989\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.3682 - val_loss: 21.4165 - learning_rate: 7.8125e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.2959 - val_loss: 21.4146 - learning_rate: 3.9062e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.3356 - val_loss: 21.4097 - learning_rate: 3.9062e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.3057 - val_loss: 21.3996 - learning_rate: 3.9062e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.3793 - val_loss: 21.4053 - learning_rate: 3.9062e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m466/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 11.2558\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.2681 - val_loss: 21.3977 - learning_rate: 3.9062e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 11.3792 - val_loss: 21.4100 - learning_rate: 1.9531e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.3344 - val_loss: 21.4118 - learning_rate: 1.9531e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 11.3041 - val_loss: 21.3976 - learning_rate: 1.9531e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.2838 - val_loss: 21.4107 - learning_rate: 1.9531e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 11.1963\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 11.2848 - val_loss: 21.4047 - learning_rate: 1.9531e-05\n",
      "Epoch 95: early stopping\n",
      "Restoring model weights from the end of the best epoch: 80.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating final model performance ---\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\n",
      "==================================================\n",
      "✅ Final SMAPE Score on Validation Set: 50.1018%\n",
      "==================================================\n",
      "⚡ Making predictions on test data...\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "✅ Predictions saved to /Users/rashidixit/Downloads/student_resource/test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 1. SETUP & DATA LOADING\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# File paths\n",
    "train_csv_path = \"/Users/rashidixit/Downloads/student_resource/dataset/train.csv\"\n",
    "train_text_emb_path = \"/Users/rashidixit/Downloads/student_resource/train_text_emb.npy\"\n",
    "train_clip_emb_path = \"/Users/rashidixit/Downloads/student_resource/train_clip_embeddings.npy\"\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(train_csv_path)\n",
    "text_features = np.load(train_text_emb_path)\n",
    "image_features = np.load(train_clip_emb_path)\n",
    "\n",
    "# Combine text and image features\n",
    "X_text = np.array(text_features)\n",
    "X_img = np.array(image_features)\n",
    "X = np.concatenate([X_text, X_img], axis=1)\n",
    "y_df = df_train[['price']]  # Load target as DataFrame for easier handling\n",
    "\n",
    "# ===================================================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# ===================================================================\n",
    "print(\"\\n--- Preprocessing Data ---\")\n",
    "\n",
    "# Combine features and target, and drop rows with NaN values\n",
    "full_df = pd.concat([pd.DataFrame(X), y_df], axis=1)\n",
    "full_df.dropna(inplace=True)\n",
    "\n",
    "X = full_df.iloc[:, :-1].values\n",
    "y_raw = full_df.iloc[:, -1].values\n",
    "print(f\"Data shape after dropping NaNs: {X.shape}\")\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "scaler_path = \"/Users/rashidixit/Downloads/student_resource/scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Log-transform the target variable\n",
    "y = np.log1p(y_raw)\n",
    "print(\"Log-transform of target variable complete.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. CUSTOM SMAPE LOSS FUNCTION\n",
    "# ===================================================================\n",
    "def smape(y_true, y_pred):\n",
    "    epsilon = 1e-6\n",
    "    numerator = tf.abs(y_pred - y_true)\n",
    "    denominator = (tf.abs(y_true) + tf.abs(y_pred) + epsilon) / 2.0\n",
    "    return 100 * tf.reduce_mean(numerator / denominator)\n",
    "\n",
    "# ===================================================================\n",
    "# 4. BUILD THE MODEL\n",
    "# ===================================================================\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_stable_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(1e-5)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_stable_model(X_scaled.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# ===================================================================\n",
    "# 5. COMPILE AND TRAIN\n",
    "# ===================================================================\n",
    "optimizer = AdamW(learning_rate=1e-2, weight_decay=1e-5)\n",
    "model.compile(optimizer=optimizer, loss=smape)\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "chk_path = \"/Users/rashidixit/Downloads/student_resource/best_model.h5\"\n",
    "chk = ModelCheckpoint(chk_path, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n--- Starting Training on {len(X_train)} samples ---\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr, early_stop, chk]\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. FINAL EVALUATION\n",
    "# ===================================================================\n",
    "print(\"\\n--- Evaluating final model performance ---\")\n",
    "best_model = tf.keras.models.load_model(chk_path, custom_objects={'smape': smape})\n",
    "\n",
    "val_preds_log = best_model.predict(X_val)\n",
    "val_preds_original = np.expm1(val_preds_log.flatten())\n",
    "y_val_original = np.expm1(y_val)\n",
    "\n",
    "def smape_numpy(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "final_smape_score = smape_numpy(y_val_original, val_preds_original)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"✅ Final SMAPE Score on Validation Set: {final_smape_score:.4f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ===================================================================\n",
    "# 7. PREDICT ON TEST DATA\n",
    "# ===================================================================\n",
    "# Load test data\n",
    "test_text_emb_path = \"/Users/rashidixit/Downloads/student_resource/test_text_emb.npy\"\n",
    "test_clip_emb_path = \"/Users/rashidixit/Downloads/student_resource/test_clip_embeddings.npy\"\n",
    "test_csv_path = \"/Users/rashidixit/Downloads/student_resource/dataset/test.csv\"\n",
    "\n",
    "test_text_features = np.load(test_text_emb_path)\n",
    "test_image_features = np.load(test_clip_emb_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Combine test features\n",
    "X_test = np.concatenate([test_text_features, test_image_features], axis=1)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predict\n",
    "print(\"⚡ Making predictions on test data...\")\n",
    "test_preds_log = best_model.predict(X_test_scaled)\n",
    "test_preds = np.expm1(test_preds_log.flatten())\n",
    "\n",
    "# Save predictions\n",
    "output_file = \"/Users/rashidixit/Downloads/student_resource/test_predictions.csv\"\n",
    "submission = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': test_preds})\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"✅ Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31a976f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"test_predictions.csv\").isna().sum()\n",
    "df = pd.read_csv(\"test_predictions.csv\")\n",
    "df['price'] = df['price'].fillna(df['price'].median())\n",
    "df.to_csv(\"test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16a81953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id    0\n",
       "price        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test_predictions.csv\").isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a6241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
